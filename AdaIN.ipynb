{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport os\nfrom skimage import io\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2022-03-14T10:26:09.634388Z","iopub.execute_input":"2022-03-14T10:26:09.635108Z","iopub.status.idle":"2022-03-14T10:26:12.349466Z","shell.execute_reply.started":"2022-03-14T10:26:09.634992Z","shell.execute_reply":"2022-03-14T10:26:12.348356Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nnum_epochs = 20\nbatch_size = 8\nlearning_rate = 1e-4\nlamda = 0.01\ncontent_path = '../input/style-transfer/CelebAMask-HQ/CelebAMask-HQ/CelebA-HQ-img/'\nstyle_path = '../input/style-transfer/style/style/'","metadata":{"execution":{"iopub.status.busy":"2022-03-14T10:32:49.864985Z","iopub.execute_input":"2022-03-14T10:32:49.865306Z","iopub.status.idle":"2022-03-14T10:32:49.871304Z","shell.execute_reply.started":"2022-03-14T10:32:49.865275Z","shell.execute_reply":"2022-03-14T10:32:49.870135Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class ContentDataset(data.Dataset):\n    def __init__(self, transform=transforms.ToTensor()):\n        super().__init__()\n        self.content_images = [f for f in os.listdir(content_path)]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        content_img = io.imread(content_path + self.content_images[index])\n        content_img = self.transform(content_img)\n        return content_img\n\n    def __len__(self):\n        return len(self.content_images) \n\nclass StyleDataset(data.Dataset):\n    def __init__(self, transform=transforms.ToTensor()):\n        super().__init__()\n        self.style_images = [f for f in os.listdir(style_path)]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        style_img = io.imread(style_path + self.style_images[index])\n        style_img = self.transform(style_img)\n        return style_img\n\n    def __len__(self):\n        return len(self.style_images) \n\ntransform = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Resize(512),\n                                        transforms.RandomCrop(256)])\ncontent_dataset = ContentDataset(transform=transform)\nstyle_dataset = StyleDataset(transform=transform)\ncontent_loader = data.DataLoader(dataset=content_dataset, batch_size=batch_size, shuffle=True)\nstyle_loader = data.DataLoader(dataset=style_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T10:32:51.088420Z","iopub.execute_input":"2022-03-14T10:32:51.088907Z","iopub.status.idle":"2022-03-14T10:32:51.120118Z","shell.execute_reply.started":"2022-03-14T10:32:51.088871Z","shell.execute_reply":"2022-03-14T10:32:51.119157Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def mu(x):\n    size = x.size()\n    assert (len(size) == 4)\n    N, C = size[:2]\n    return x.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n\ndef sigma(x, eps=1e-5):\n    size = x.size()\n    assert (len(size) == 4)\n    N, C = size[:2]\n    var = x.view(N, C, -1).var(dim=2) + eps\n    std = var.sqrt().view(N, C, 1, 1)\n    return std","metadata":{"execution":{"iopub.status.busy":"2022-03-14T10:28:51.834991Z","iopub.execute_input":"2022-03-14T10:28:51.835839Z","iopub.status.idle":"2022-03-14T10:28:51.845808Z","shell.execute_reply.started":"2022-03-14T10:28:51.835788Z","shell.execute_reply":"2022-03-14T10:28:51.844637Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class AdaIN(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x, y):\n        assert (x.size()[:2] == y.size()[:2])\n        size = x.size()\n        style_mean, style_std = mu(y), sigma(y)\n        content_mean, content_std = mu(x), sigma(x)\n        normalized_feat = (x - content_mean.expand(size)) / content_std.expand(size)\n        return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = [nn.Conv2d(3, 3, 1)]\n        vgg_features = models.vgg19(pretrained=True).features.children()\n        for layer in vgg_features:\n            layers.append(layer)\n            if isinstance(layer, torch.nn.Conv2d):\n                layer.padding_mode = 'reflect'\n\n        self.net = nn.Sequential(*(layers[: 22]))\n    \n    def forward(self, x):\n        phi_1, phi_2 ,phi_3 ,phi_4 = None, None, None, None\n        for i, layer in enumerate(self.net):          \n            x = layer(x)\n            if i == 2:      \n                phi_1 = x\n            elif i == 7:\n                phi_2 = x\n            elif i == 12:\n                phi_3 = x\n            elif i == 21: \n                phi_4 = x\n\n        return phi_1, phi_2, phi_3, phi_4\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(512, 256, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(256, 256, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Conv2d(256, 128, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(64, 64, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU(),\n            nn.Conv2d(64, 3, 3, padding=1, padding_mode='reflect'),\n            nn.ReLU())\n\n    def forward(self, x):\n        return self.net(x)\n\nadain = AdaIN().to(device)\ndecoder = Decoder().to(device)\nencoder = Encoder().to(device)\nfor param in encoder.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-03-14T10:28:53.882922Z","iopub.execute_input":"2022-03-14T10:28:53.883397Z","iopub.status.idle":"2022-03-14T10:29:15.599260Z","shell.execute_reply.started":"2022-03-14T10:28:53.883362Z","shell.execute_reply":"2022-03-14T10:29:15.598309Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"decoder.train()\nmse = nn.MSELoss()\noptimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    for c, s in zip(content_loader, style_loader):\n        c = c.to(device) \n        s = s.to(device)\n        _, _, _, f_c = encoder(c)\n        phi_1_s, phi_2_s, phi_3_s, phi_4_s = encoder(s)\n        t = adain(f_c, phi_4_s)\n        g = decoder(t)\n        phi_1_g, phi_2_g, phi_3_g, phi_4_g = encoder(g)\n        content_loss = mse(phi_4_g, t)\n        mu_loss = mse(mu(phi_1_g), mu(phi_1_s)) + mse(mu(phi_2_g), mu(phi_2_s)) + mse(mu(phi_3_g), mu(phi_3_s)) + mse(mu(phi_4_g), mu(phi_4_s))\n        std_loss = mse(sigma(phi_1_g), sigma(phi_1_s)) + mse(sigma(phi_2_g), sigma(phi_2_s)) + mse(sigma(phi_3_g), sigma(phi_3_s)) + mse(sigma(phi_4_g), sigma(phi_4_s))\n        style_loss = mu_loss + std_loss\n        loss = content_loss + lamda * style_loss\n        epoch_loss = epoch_loss + loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    print('epoch:', epoch, 'loss:', epoch_loss)\n    c = c.to(device) \n    s = s.to(device)\n    _, _, _, f_c = encoder(c)\n    _, _, _, phi_4_s = encoder(s)\n    t = adain(f_c, phi_4_s)\n    g = decoder(t)\n    content = c[0].permute(1, 2, 0).cpu().detach().numpy()\n    style = s[0].permute(1, 2, 0).cpu().detach().numpy()\n    out = g[0].permute(1, 2, 0).cpu().detach().numpy()\n    plt.imshow(content)\n    plt.show()\n    plt.imshow(style)\n    plt.show()\n    plt.imshow(out)\n    plt.show()\n    torch.save(decoder.state_dict(), './params.pt')\n    torch.save(optimizer.state_dict(), './opt.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}